# -*- coding: utf-8 -*-
"""Machine_Learning_to_Predict_Credit_Risk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QqrCnwI5b7xxtTCw66hQiwsnWv49z0r6

<a href="https://colab.research.google.com/github/nurayuasyifa/credit-risk-analysis-and-prediction/blob/main/Machine_Learning_to_Predict_Credit_Risk.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Business Understanding

- projek dari sebuah lending company
- membangun model yang dapat memprediksi credit risk 
- menggunakan dataset yang disediakan oleh company yang terdiri dari **data pinjaman yang diterima dan yang ditolak**.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('max_columns',None)

loan = pd.read_csv('/content/drive/MyDrive/Rakamin/loan_data_2007_2014.csv',low_memory=False)

loan.shape

loan.drop(columns='Unnamed: 0',inplace=True)

loan1 = loan.copy()

"""bad debt :
* Charged off
* Default
* Late (31-120 days)
* Late (16-30 days)
* Does not meet the credit policy. Status:Charged Off

good debt:
* In Grace Period
* Fully Paid
* Current
* Does not meet the credit policy. Status:Fully Paid
"""

# membuat feature target
loan1['target'] = np.where((loan1['loan_status'] =='Charged Off') | 
                         (loan1['loan_status'] =='Default') | 
                         (loan1['loan_status'] =='Late (31-120 days)') | 
                         (loan1['loan_status'] =='Late (16-30 days)') | 
                         (loan1['loan_status'] =='Does not meet the credit policy. Status:Charged Off'),1,0)

dfg = loan1.groupby('target').agg({'id':'count'}).reset_index()
dfg.columns = ['target','total']
dfg['%'] = round(dfg['total']*100/sum(dfg['total']),3)
dfg

plt.pie(dfg['total'],labels=dfg['target'], autopct='%.3f%%')
plt.show()

"""Berdasarkan tabel diatas persentase `bad debt rate = 11.192%`. Pembuatan model machine learning bertujuan untuk menurunkan `default rate` hingga `<5%`.

"""

loan1.groupby('target').agg({'funded_amnt':'sum','total_pymnt':'sum'}).reset_index()

"""Perusahaan loss sebesar $760,916,150 karena 11.192% peminjam tidak mampu bayar

# Data Collection

feature yang semua nilainya null di drop untuk memudahkan proses selanjutnya
"""

to_drop = loan1.isnull().sum().sort_values()
to_drop = to_drop[to_drop == loan1.shape[0]]
to_drop = list(to_drop.index)

#check dimensi dataset
print('dimensi dataset sebelum drop = ',loan1.shape)

#drop feature yang semua isinya nilai null
loan1.drop(columns=to_drop,inplace=True)

#check dimensi dataset
print('dimensi dataset setelah drop = ',loan1.shape)

# memisahkan feature numerical dengan categorical
nums = []
cats = []
for i in loan1.columns:
  if loan1[i].dtype == 'object':
    cats.append(i)
  else:
    nums.append(i)
print('jumlah = ',len(nums))
print('nums = ',nums)
print('jumlah = ',len(cats))
print('cats = ',cats)

loan1['funded_amnt'][loan1['target']==1].sum()

"""## **Statistical Summary For Categorical Features**"""

loan1[cats].describe().transpose()

"""**Observasi:**
* Features `emp_title`,`url`,`desc`,`title`,`zip_code`,`addr_state` tidak akan digunakan karena terlalu banyak nilai yang unik
* Feature `application_type` juga tidak akan digunakan karena hanya memiliki 1 nilai unik, sehingga tidak ada informasi yang dapat diperoleh dari feature tersebut.
"""

#check dimensi dataset
print('dimensi dataset sebelum drop = ',loan1.shape)

#drop
loan1.drop(columns=['emp_title','url','desc','title','zip_code','addr_state','application_type'],inplace=True)

#check dimensi dataset
print('dimensi dataset setelah drop = ',loan1.shape)

"""## **Statistical Summary for Numerical Features**"""

loan1[nums].describe().transpose()

"""**Keterangan:**
* `id` adalah ID yang ditetapkan LC unik untuk daftar pinjaman
* `member_id` adalah ID unik yang ditetapkan LC untuk anggota peminjam
* `loan_amnt` jumlah yang diajukan oleh peminjam
* `funded_amnt` jumlah yg diberikan kepada peminjam
* `funded_amnt_inv` jumlah yang diberikan investor untuk peminjam
* `int_rate` bunga pinjaman

* `annual_inc` adalah income tahunan peminjam
* `dti` adalah rasio antara pinjaman dengan income
* `delinq_2yrs` adalah Jumlah insiden tunggakan 30 hari dalam arsip kredit peminjam selama 2 tahun terakhir
*  `inq_last_6mnths` adalah jumlah penyelidikan selama 6 bulan terakhir (kecuali mobil dan hipotek)
*  `mths_since_last_delinq` adalah Jumlah bulan sejak tunggakan terakhir peminjam.
* `mths_since_last_record` adalah Jumlah bulan sejak catatan publik terakhir

* `pub_rec` adalah jumlah catatan publik yg jatuh tempo
* `revol_bal` adalah total saldo kredit revolving (porsi pengeluaran kartu kredit yang tidak dibayar pada akhir siklus penagihan)
* `revol_util` adalah Tingkat pemanfaatan jalur bergulir, atau jumlah kredit yang digunakan peminjam relatif terhadap semua kredit bergulir yang tersedia.
* `total_acc` adalah Jumlah total jalur kredit saat ini dalam file kredit peminjam
* `out_prncp` adalah Sisa pokok pinjaman untuk jumlah total yang didanai
* `out_prncp_inv` adalah Sisa pokok pinjaman untuk sebagian dari jumlah total yang didanai oleh investor

* `total_pymnt_inv` adalah Pembayaran yang diterima hingga saat ini untuk sebagian dari jumlah total yang didanai oleh investor
* `total_rec_prncp` adalah pembayaran pokok yang diterima hingga saat ini
* `total_rec_int` adalah bunga yang diterima hingga saat ini
* `total_rec_late_fee` adalah biaya keterlambatan diterima hingga saat ini
* `recoveries` adalah pasca charge off pemulihan kotor
* `collection_recovery_fee` adalah post charge off biaya pengumpulan

* `collections_12_mths_ex_med` adalah jumlah pemungutan dalam 12 bulan terakhir tidak termasuk medis
* `mths_since_last_major_derog` adalah bulan sejak peringkat 90 hari terakhir atau lebih buruk
* `policy_code` adalah policy_code=1 tersedia untuk umum produk baru tidak tersedia untuk umum policy_code=2
* `acc_now_delinq` adalah Jumlah rekening di mana peminjam sekarang menunggak
* `tot_coll_amt` adalah jumlah total pemungutan yang pernah terhutang
* `tot_cul_bar` adalah Total saldo saat ini dari semua akun
* `total_rev_hi_lim` adalah Total kredit / batas kredit tinggi revolving/bergulir

Feature-feature berdasarkan 5C:
1. **Character :** 'loan_amnt','delinq_2yrs','mths_since_last_delinq','pub_rec','revol_bal','total_acc','total_rec_late_fee','mths_since_last_major_derog',acc_now_delinq', 'tot_coll_amt','total_rev_hi_lim','term'
2. **Capacity :** 'dti'
3. **Capital :** 'mths_since_last_delinq',
4. **Collateral :** 'home_ownership','purpose'
5. **Conditions :** 'out_prncp','total_rec_prncp','total_rec_int','int_rate','annual_inc'

**Observasi:**
* Feature `policy_code` dapat didrop karena hanya memiliki 1 nilai
"""

#check dimensi dataset
print('dimensi dataset sebelum drop = ',loan1.shape)

#drop feature yang semua isinya nilai null
loan1.drop(columns='policy_code',inplace=True)

#check dimensi dataset
print('dimensi dataset setelah drop = ',loan1.shape)

"""# Exploratory Data Analysis"""

df1 = loan1.copy()

df1.shape

# memisahkan feature numerical dengan categorical
nums1 = []
cats1 = []
for i in loan1.columns:
  if df1[i].dtype == 'object':
    cats1.append(i)
  else:
    nums1.append(i)
print('jumlah = ',len(nums1))
print('nums = ',nums1)
print('jumlah = ',len(cats1))
print('cats = ',cats1)

"""## Statistical Summary : Categorical Features"""

#menghapus spasi
df1.term=df1.term.str.lstrip()

df1[cats1].describe().transpose()

for i in cats1:
  print('-----'*10)
  print(i)
  print(df1[i].value_counts())

"""**Observasi:**
* Untuk feature `issue_d`,`earliest_cr_line`, `last_pymnt_d`, `next_pymnt_d`, dan `last_credit_pull_d` tipe datanya dapat diubah ke datetime
* Label pada feature `initial_list_status` dapat diubah ke bentuk biner
* Label 'ANY' dan 'NONE' pada feature `home_ownership` dapat digabung dengan label 'OTHER'
* Karena feature `target` merupakan feature engineering dari feature`loan_status` maka, feature `loan_status` dapat didrop

## Statistical Summary : Numerical Features
"""

df1[nums1].describe().transpose()

"""## Univariate Analysis : Categorical Features"""

to_date = []
cats2 = []
for i in cats1:
  if df1[i].nunique() > 35:
    to_date.append(i)
  else:
    cats2.append(i)
cats1 = cats2.copy()

print(cats1)
print(to_date)

#membuat urutan pada feature emp_length
length = ['< 1 year','1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years']
df1['emp_length'] = pd.Categorical(df1['emp_length'], categories=length, ordered=True)

#membuat urutan pada feature grade
temp = ['A', 'B', 'C', 'D', 'E', 'F', 'G']
df1['grade'] = pd.Categorical(df1['grade'], categories=temp, ordered=True)

#membuat urutan pada feature emy_length
temp = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1',
       'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2',
       'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5', 'G1', 'G2', 'G3',
       'G4', 'G5']
df1['sub_grade'] = pd.Categorical(df1['sub_grade'], categories=temp, ordered=True)

"""### **Feature Term**"""

term = df1.groupby('term').agg({'id':'count'}).reset_index()
term.columns=['term','total']
term['%'] = round(term.total*100/sum(term.total),3)
term

plt.figure(figsize=(5,5))
sns.countplot(x=df1['term'])
plt.text(x=-0.2,y=337953,s='72.478%',fontsize=14)
plt.text(x=0.8,y=128332,s='27.522%',fontsize=14)
plt.show()

"""Feature `term` didominasi oleh label '36 month' dengan persentase 72.478% dari total populasi.

### **Feature grade dan sub_grade**
"""

grade = df1.groupby('grade').agg({'id':'count'}).reset_index()
grade.columns=['grade','total']
grade['%'] = round(grade.total*100/sum(grade.total),3)
grade

plt.figure(figsize=(5,5))
sns.countplot(x=df1['grade'])
plt.show()

"""Feature `grade` didominasi oleh label 'B' dan 'C' yang masing-masing memiliki persentase 29.366% dan 26.870% dari jumlah total populasi"""

sub_grade = df1.groupby('sub_grade').agg({'id':'count'}).reset_index()
sub_grade.columns=['sub_grade','total']
sub_grade['%'] = round(sub_grade.total*100/sum(sub_grade.total),3)
sub_grade

plt.figure(figsize=(10,5))
sns.countplot(x=df1['sub_grade'])
plt.show()

"""Pada feature `sub_grade` didominasi oleh label 'B2' dan 'B3' dengan persentase masing-masing sebesar 5.707% dan 6.795% dari jumlah total populasi

### **Feature emp_length**
"""

emp_length = df1.groupby('emp_length').agg({'id':'count'}).reset_index()
emp_length.columns=['emp_length','total']
emp_length['%'] = round(emp_length.total*100/sum(emp_length.total),3)
emp_length

plt.figure(figsize=(10,5))
sns.countplot(x=df1['emp_length'])
plt.show()

"""Feature `emp_length` didominasi dengan label '10+ years' dengan persentase sebesar 33.698% dari jumlah total populasi

### **Feature home_ownership**
"""

home_ownership = df1.groupby('home_ownership').agg({'id':'count'}).reset_index()
home_ownership.columns=['home_ownership','total']
home_ownership['%'] = round(home_ownership.total*100/sum(home_ownership.total),3)
home_ownership=home_ownership.sort_values('%',ascending=False)
home_ownership

plt.figure(figsize=(10,5))
sns.barplot(x=home_ownership['home_ownership'],y=home_ownership['%'])
plt.show()

"""Feature home_ownership didominasi oleh label MORTGAGE dengan persentase 50.586% dari jumlah total populasi. Label NONE dan ANY dapat digabungkan dengan label OTHER

### **Feature pymnt_plan**
"""

pymnt_plan = df1.groupby('pymnt_plan').agg({'id':'count'}).reset_index()
pymnt_plan.columns=['pymnt_plan','total']
pymnt_plan['%'] = round(pymnt_plan.total*100/sum(pymnt_plan.total),3)
pymnt_plan=pymnt_plan.sort_values('%',ascending=False)
pymnt_plan

plt.figure(figsize=(5,5))
sns.barplot(x=pymnt_plan['pymnt_plan'],y=pymnt_plan['%'])
plt.show()

"""Feature `pymnt_plan` didominasi oleh label n dengan persentase sebesar 99.998% dari jumlah total populasi. Imbalance ekstreme

### **Feature Purpose**
"""

purpose = df1.groupby('purpose').agg({'id':'count'}).reset_index()
purpose.columns=['purpose','total']
purpose['%'] = round(purpose.total*100/sum(purpose.total),3)
purpose=purpose.sort_values('%',ascending=False)
purpose

plt.figure(figsize=(10,5))
sns.barplot(x=purpose['purpose'],y=purpose['%'])
plt.xticks(rotation=90)
plt.show()

"""Feature `purpose` didominasi label debt_consolidation dengan persentase 58.804% dari jumlah total populasi

### **Feature initial_list_status**
"""

initial_list_status = df1.groupby('initial_list_status').agg({'id':'count'}).reset_index()
initial_list_status.columns=['initial_list_status','total']
initial_list_status['%'] = round(initial_list_status.total*100/sum(initial_list_status.total),3)
initial_list_status=initial_list_status.sort_values('%',ascending=False)
initial_list_status

plt.figure(figsize=(5,5))
sns.barplot(x=initial_list_status['initial_list_status'],y=initial_list_status['%'])
plt.show()

"""Feature `initial_list_status` didominasi oleh label f dengan presentase sebesar 64.983% dari jumlah total populasi

**Observasi:**
* Feature `term` didominasi oleh label 36 months
* Feature `grade` didominasi oleh label B dan C
* Feature `sub_grade`didominasi oleh sub dari label B dan C
* Feature `emp_length` didominasi oleh user yang bekerja 10 tahun ke atas
* Feature `home_ownership` didominasi oleh label MORTGAGE. Label NONE dan ANY dapat digabungkan dengan label OTHER
* Feature `verification_status` label Verified paling tinggi
* Feature `pymnt_plan` didominasi label n (imbalance ekstrem)
* Feature `purpose` didominasi label debt_consolidation
* Feature `initial_list_status` didominasi label f

### **Feature Engineering : Date**
"""

print(to_date)

df1[to_date].head()

"""Memisahkan feature month dan year"""

df1['issue_d_month'] = df1.issue_d.str[:3]
df1['issue_d_year'] = np.where(df1.issue_d.str[4:].astype('float64')>30,'19'+ df1.issue_d.str[4:],'20'+ df1.issue_d.str[4:])

df1['earliest_cr_line_month'] = df1.earliest_cr_line.str[:3]
df1['earliest_cr_line_year'] = np.where(df1.earliest_cr_line.str[4:].astype('float64')>30,'19'+ df1.earliest_cr_line.str[4:],'20'+ df1.earliest_cr_line.str[4:])

df1['last_pymnt_d_month'] = df1.last_pymnt_d.str[:3]
df1['last_pymnt_d_year'] = np.where(df1.last_pymnt_d.str[4:].astype('float64')>30,'19'+ df1.last_pymnt_d.str[4:],'20'+ df1.last_pymnt_d.str[4:])

df1['next_pymnt_d_month'] = df1.next_pymnt_d.str[:3]
df1['next_pymnt_d_year'] = np.where(df1.next_pymnt_d.str[4:].astype('float64')>30,'19'+ df1.next_pymnt_d.str[4:],'20'+ df1.next_pymnt_d.str[4:])

df1['last_credit_pull_d_month'] = df1.last_credit_pull_d.str[:3]
df1['last_credit_pull_d_year'] = np.where(df1.last_credit_pull_d.str[4:].astype('float64')>30,'19'+ df1.last_credit_pull_d.str[4:],'20'+ df1.last_credit_pull_d.str[4:])

df1[['issue_d_month','issue_d_year', 'earliest_cr_line_month', 'earliest_cr_line_year',
     'last_pymnt_d_month', 'last_pymnt_d_year', 'next_pymnt_d_month','next_pymnt_d_year', 
     'last_credit_pull_d_month','last_credit_pull_d_year']].head()

df1[['issue_d','issue_d_month','issue_d_year',
     'earliest_cr_line','earliest_cr_line_month','earliest_cr_line_year',
     'last_pymnt_d','last_pymnt_d_month','last_pymnt_d_year',
     'next_pymnt_d','next_pymnt_d_month','next_pymnt_d_year',
     ]].isnull().sum()

#drop kolom yang ada pada list to_date
#df1.drop(columns=to_date,inplace=True)

#membuat urutan pada feature emy_length
temp = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
month = ['issue_d_month', 'earliest_cr_line_month','last_pymnt_d_month','next_pymnt_d_month','last_credit_pull_d_month']
for i in month:
  df1[i] = pd.Categorical(df1[i], categories=temp, ordered=True)

"""**Feature issue_d_year**"""

a = df1.groupby(['issue_d_year']).agg({'id':'count'}).rename(columns={'id':'total'}).reset_index()
 a['%'] = round(a['total']*100/sum(a['total']),3)
 a

plt.figure(figsize=(12,5))
sns.countplot(df1['issue_d_year'].sort_values())
plt.show()

"""Pada feature `issue_d_year` 50.533% dari jumlah total populasi didominasi oleh label '2014'.

**Feature earliest_cr_line_year**
"""

b = df1.groupby(['earliest_cr_line_year']).agg({'id':'count'}).rename(columns={'id':'total'}).reset_index()
b['%'] = round(b['total']*100/sum(b['total']),3)
b

plt.figure(figsize=(12,5))
sns.countplot(df1['earliest_cr_line_year'].sort_values())
plt.xticks(rotation=90)
plt.show()

"""Feature `earlies_cr_line_year` distribusi datanya negatively skewed

**Feature last_pymnt_d_year**
"""

c = df1.groupby(['last_pymnt_d_year']).agg({'id':'count'}).rename(columns={'id':'total'}).reset_index()
c['%'] = round(c['total']*100/sum(c['total']),3)
c

plt.figure(figsize=(12,5))
sns.countplot(df1['last_pymnt_d_year'].sort_values())
plt.xticks(rotation=90)
plt.show()

"""Pada feature `last_pymnt_d_year` 38.553% dari jumlah total populasi didominasi oleh label tahun 2016

**Feature next_pymnt_d_year**
"""

d = df1.groupby(['next_pymnt_d_year']).agg({'id':'count'}).rename(columns={'id':'total'}).reset_index()
d['%'] = round(d['total']*100/sum(d['total']),3)
d

plt.figure(figsize=(12,5))
sns.countplot(df1['next_pymnt_d_year'].sort_values())
plt.xticks(rotation=90)
plt.show()

"""Feature `next_payment_d_year` 98.854% dari jumlah total populasi didominasi oleh label tahun 2016

**Feature last_credit_pull_d_year**
"""

d = df1.groupby(['last_credit_pull_d_year']).agg({'id':'count'}).rename(columns={'id':'total'}).reset_index()
d['%'] = round(d['total']*100/sum(d['total']),3)
d

plt.figure(figsize=(12,5))
sns.countplot(df1['last_credit_pull_d_year'].sort_values())
plt.xticks(rotation=90)
plt.show()

"""Feature `last_credit_pull_d_year` 70.285% dari jumlah total populasi didominasi oleh label tahun 2016

## Univariate Analysis : Numerical Features
"""

print(nums1)

len(nums1)

temp = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 
        'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq', 
        'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 
        'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 
        'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 
        'collection_recovery_fee', 'last_pymnt_amnt', 'collections_12_mths_ex_med', 
        'mths_since_last_major_derog', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 
        'total_rev_hi_lim']

len(temp)

"""### Distribution Numerical Features"""

plt.figure(figsize=(15,15))
for i in range(0, len(temp)):
    plt.subplot(8, 4, i+1)
    sns.boxplot(x=df1[temp[i]], color='royalblue')
    plt.xlabel(temp[i])
    plt.tight_layout()

df1[temp].columns

"""**Observasi:**
* Feature yang tidak memiliki outliers `'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'dti', 'mths_since_last_record'`
* Feature `acc_now_delinq` merupakan data ordinal

## Bivariate Analysis : Categorical Features
"""

print(cats1)

temp = ['term', 'grade','emp_length', 'home_ownership', 'verification_status',
        'pymnt_plan', 'purpose', 'initial_list_status']

plt.figure(figsize=(15,8))
for i in range(0, len(temp)):
    plt.subplot(2, 4, i+1)
    sns.countplot(df1[temp[i]], hue=df1['target'])
    plt.xlabel(temp[i])
    plt.xticks(rotation=90)
    plt.tight_layout()

"""**Observasi:**
* Pada feature `term` jumlah tertinggi dari peminjam yang aggal abayr ada pada kategori `term = 36 months`.
* jumlah peminjam yang gagal bayar terbanyak pada feature `grade` adalah pelanggan dengan `grade = C`
* jumlah peminjam yang gagal bayar terbanyak pada feature `emp_length` adalah pelanggan dengan masa kerja lebih dari 10 tahun. Sedangkan untuk pelanggan dengan masa kerja kurang dari 10 tahun, jumlah peminjam yang gagal bayar rerata cukup sama jumlahnya.
* Berdasarkan `home_ownership` jumlah peminjam yang gagal bayar terbanyak adalah pelanggan dengan tipe `home_ownership` 'RENT' dan 'MORTGAGE'.
* Berdasarkan `verification_status` jumlah peminjam yang gagal bayar paling banyak pada satatus 'Verified'. Sedangkan untuk status yang lain, peminjam yang gagal bayar memiliki rata-rata jumlah yang sama.
* Berdasarkan feature `purpose` jumlah peminjam yang gagal bayar paling banyak ada pada tujuan peminjaman untuk 'debt_consolidation'.
* Berdasarkan feature `initial_list_status` jumlah peminjam yang gagal bayar paling banyak adalh kategori 'f'.
"""

temp = ['issue_d_year','last_pymnt_d_year','next_pymnt_d_year','last_credit_pull_d_year'] #'earliest_cr_line_year'

plt.figure(figsize=(20,5))
for i in range(0, len(temp)):
    plt.subplot(1, 4, i+1)
    sns.countplot(df1[temp[i]].sort_values(), hue=df1['target'])
    plt.xlabel(temp[i])
    plt.xticks(rotation=90)
    plt.tight_layout()

"""**Observasi:**
* Pada feature `issue_d_year` jumlah peminjam yang mampu dan gagal bayar meningkat setiap tahunnya. Jumlah total peminjam yang gagal bayar paling tinggi ada pada tahun 2014.
* Pada feature `last_pymnt_d_year` jumlah peminjam yang mampu dan gagal bayar meningkat setiap tahunnya. Akan tetapi, pada tahun 2016, Jumlah peminjam yang gagal bayar jumlahnya menurun.
* Pada feature `next_pymnt_d_year` jumlah peminjam yang gagal bayar tertinggi ada pada tahun 2016
* Pada feature `last_credit_pull_d_year` jumlah peminjam yang mampu dan gagal bayar setiap tahunnya meningkat. 
"""

plt.figure(figsize=(20,5))
sns.countplot(df1['earliest_cr_line_year'].sort_values(), hue=df1['target'])
plt.xticks(rotation=90)
plt.show()

"""Pada feature `earliest_cr_line_year` jumlah peminjam yag mampu bayar berbading lurus dengan yang gagal bayar. Jika jumlah peminjam yang mampu bayar meningkat maka jumlah yang gagal bayar pun akan meningkat.

## Bivariate Analysis : Numerical Features
"""

print(nums1)

temp = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_amnt', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']

print(temp)

len(temp)

df_sample=df1.sample(1000,random_state=1)
plt.figure(figsize=(15,20))
for i in range(0, len(temp)):
    plt.subplot(8, 4, i+1)
    sns.boxplot(y=df_sample[temp[i]], x=df1['target'])
    plt.xlabel(temp[i])
    plt.tight_layout()

"""**Observasi:**
* Peminjam dengan nilai `total_rec_prncp` diantara 0 hingga 5000 adalah peminjam yang gagal bayar. Sedangkan peminjam dengan nilai `total_rec_prncp` lebih dari 5000 adlaah peminjam yang mampu bayar
* Pada feature `total_rec_int` peminjam yang mampu dan gagal bayar ada pada rentang yang sama, yaitu antara 0 hingga 5000. 
* Peminjam yang gagal bayar memiliki nilai `total_rec_late_fee`yang lebih tinggi dibandingkan peminjam yang mampu bayar
* Peminjam dengan nilai `recoveries`> 0 merupakan peminjam yang gagal bayar. Sedangkan peminjam dengan `recoveries` = 0 merupakan peminjam yang mampu bayar.
* Peminjam dengan `collection_recovery_fee` > 0 adalah peminjam yang gagal bayar. Sedangkan, peminjam dengan `collection_recovery_fee` = 0 adalah peminjam yang dapat bayar.
* Pada feature `last_pymnt_amnt` peminjam yang total pembayaran terakhir (`last_pymnt_amnt`) < 10000 adalah peminjam yang gagal bayar.
* Peminjam dengan nilai `mnth_since_Last_major_derog` diantara 0 hingga 75 adalah peminjam yang gagal bayar.
* Peminjam dengan nilai `tot_cur_bal` diantara 0 hingga 0.5 adalah peminjam yang gagal bayar. 
* Peminjam dengan nilai `total_rev_hi_lim` diantara 0 hingga 150000 adalah peminjam yang gagal bayar.

## Multivariate Analysis
"""

plt.figure(figsize=(15,8))
sns.heatmap(df1.corr(),fmt='2.f')
plt.show()

"""## Insight

Menghitung sisa pinjaman pokok dari tiap peminjam
"""

df1[df1['target']==1].head(1)

a = df1[df1['target']==1][['loan_amnt','funded_amnt','term','int_rate','installment',
                       'out_prncp','out_prncp_inv','total_pymnt',
                       'total_pymnt_inv','total_rec_prncp','total_rec_int',
                       'total_rec_late_fee','recoveries','collection_recovery_fee']]
a['must_payment'] = np.where(a['term']=='60 months',a['installment']*60,a['installment']*36)
a.head(3)

#sisa yang harus dibayar oleh peminjam
a['dif'] = a['must_payment'] - (a['total_pymnt']+a['total_rec_late_fee']+a['collection_recovery_fee'])
a.head(3)

"""* `out_prncp`: Sisa pokok pinjaman untuk jumlah total yang didanai
* `out_prncp_inv`: Sisa pokok pinjaman untuk sebagian dari jumlah total yang didanai oleh investor
* `total_pymt`: Pembayaran diterima hingga saat ini untuk jumlah total yang didanai
* `total_rec_prncp`: Pinjaman pokok yang diterima hingga saat ini
* `total_rec_int`: Bunga yang diterima hingga saat ini

"""

round(sum(a.dif),3)

a.shape

"""Feature `must_payment` merupakan jumlah yang harus dibayar oleh peminjam. Feature `dif` merupakan selisih dari `must_payment`dan `total_pymnt` yang berarti seseorang masih memiliki kewajiban membayar sebesar `dif`. Perusahaan akan berpotensi loss sebesar $460,592,124.909 karena 52186 peminjam merupakan peminjam yang buruk (tidak mampu/gagal bayar)

### total_rec_prncp
"""

ma1 = df1.groupby(['target','purpose']).agg({'total_rec_prncp':'mean'}).reset_index().sort_values(['total_rec_prncp'],ascending=False)
ma1.columns = ['target','purpose','avg_total_rec_prncp']
ma1

plt.figure(figsize=(10,5))
sns.barplot(x='purpose',y=ma1['avg_total_rec_prncp'].sort_values(ascending=False),data=ma1[ma1['target']==1],color='cyan')
plt.text(x=-1,y=4500,s='Berdasarkan peminjam yang gagal bayar\nRata-rata total rec prncp tertinggi dengan tujuan pinjaman untuk credit card',fontsize=14)
plt.xticks(rotation=90)
plt.show()

"""Rata-rata `total_rec_prncp` peminjam yang mampu bayar lebih besar dibandingkan dengan yang gagal bayar."""

ma2 = df1.groupby(['target','grade']).agg({'total_rec_prncp':'mean'}).reset_index().sort_values(['total_rec_prncp'],ascending=False)
ma2.columns = ['target','grade','avg_total_rec_prncp']
ma2

plt.figure(figsize=(10,5))
sns.barplot(x='grade',y='avg_total_rec_prncp',data=ma2[ma2['target']==1],color='orange')
plt.text(x=-1,y=5000,s='Berdasarkan peminjam yang gagal bayar\nRata-rata tertinggi dari total rec prncp ada pada grade A',fontsize=14)
plt.show()

ma3 = df1.groupby(['target','grade','purpose']).agg({'total_rec_prncp':'mean'}).reset_index()
ma3.columns = ['target','grade','purpose','avg_total_rec_prncp']
ma3[ma3['target']==1].sort_values(['avg_total_rec_prncp'],ascending=False)

plt.figure(figsize=(15,5))
sns.barplot(x='grade',y='avg_total_rec_prncp',hue='purpose',data=ma3[ma3['target']==1],palette='coolwarm')
plt.title('Rata-rata tota rec prncp peminjam yang gagal bayar berdasarkan grade dan purpose',fontsize=14)
plt.legend(bbox_to_anchor=(1,1))
plt.show()

"""Jika peminjam yang gagal bayar ditinjau dari `grade`,`purpose`, dan rata-rata `total_rec_prncp` maka peminjam yang ada pada kategori `grade` A dengan tujuan pinjaman untuk rumah. Di posisi kedua dan ketiga juga ada pada kategori `grade` A dengan tujuan pinjaman secara berturut-turut untuk perbaikan rumah dan kartu kredit. Sehingga dapat disimpulkan bahwa, untuk mengurangi peminjam yang gagal bayar dapat mempertimbangkan untuk menolak peminjam yang berada di `grade` A."""

df1.groupby('target').agg({'total_rec_prncp':'mean'})

"""### collection_recovery_fee"""

ma4 = df1.groupby(['target','purpose']).agg({'collection_recovery_fee':'mean'}).reset_index().sort_values(['collection_recovery_fee'],ascending=False)
ma4.columns = ['target','purpose','avg_collection_recovery_fee']
ma4[ma4['target']==1]

plt.figure(figsize=(10,5))
sns.barplot(x='purpose',y=ma4['avg_collection_recovery_fee'].sort_values(ascending=False),data=ma4[ma4['target']==1],color='cyan')
plt.text(x=-1,y=160,s='Berdasarkan peminjam yang gagal bayar\nRata-rata collection recovery fee tertinggi dengan tujuan pinjaman untuk renewable energy',fontsize=14)
plt.xticks(rotation=90)
plt.show()

ma5 = df1.groupby(['target','grade']).agg({'collection_recovery_fee':'mean'}).reset_index().sort_values(['collection_recovery_fee'],ascending=False)
ma5.columns = ['target','grade','avg_collection_recovery_fee']
ma5[ma5['target']==1]

plt.figure(figsize=(10,5))
sns.barplot(x='grade',y='avg_collection_recovery_fee',data=ma5[ma5['target']==1],color='orange')
plt.text(x=-1,y=180,s='Berdasarkan peminjam yang gagal bayar\nRata-rata tertinggi dari collection_recovery_fee ada pada grade G',fontsize=14)
plt.show()

ma6 = df1.groupby(['target','grade','purpose']).agg({'collection_recovery_fee':'mean'}).reset_index()
ma6.columns = ['target','grade','purpose','avg_collection_recovery_fee']
ma6[ma6['target']==1]

plt.figure(figsize=(15,5))
sns.barplot(x='grade',y='avg_collection_recovery_fee',hue='purpose',data=ma6[ma6['target']==1],palette='coolwarm')
plt.title('Rata-rata collection_recovery_fee peminjam yang gagal bayar berdasarkan grade dan purpose',fontsize=14)
plt.legend(bbox_to_anchor=(1,1))
plt.show()

"""Jika peminjam yang gagal bayar ditinjau dari feature `grade`, `purpose`, dan rata-rata `collection_recovery_fee`, maka peminjam yang berisiko untuk gagal bayar tertinggi ada pada kategori `grade` G dengan tujuan peminjaman untuk energi terbarukan. Di posisi ke dua tertinggi, risiko peminjam yang gagal bayar ada pada kategori `grade` F dengan tujuan peminjaman untuk pernikahan. Di posisi ketiga tertinggi, risiko peminjam yang akakn gagal bayar ada pada kategori `grade` G dengan tujuan peminjaman untuk perbaikan rumah.  Sehingga dapat dibertimbangkan untuk peminjam yang ada pada kategori tersebut untuk ditolak peminjaman kreditnya.

### recoveries
"""

ma7 = df1.groupby(['target','purpose']).agg({'recoveries':'mean'}).reset_index().sort_values(['recoveries'],ascending=False)
ma7.columns = ['target','purpose','avg_recoveries']
ma7[ma7['target']==1]

plt.figure(figsize=(10,5))
sns.barplot(x='purpose',y=ma7['avg_recoveries'].sort_values(ascending=False),data=ma7[ma7['target']==1],color='cyan')
plt.text(x=-1,y=1100,s='Berdasarkan peminjam yang gagal bayar\nRata-rata recoveries tertinggi ada pada tujuan pinjaman untuk energi terbarukan',fontsize=14)
plt.xticks(rotation=90)
plt.show()

"""* Jika ditinjau berdasarkan rata-rata `recoveries` dan tujuan peminjaman (`purpose`), peminjam yang ada pada kategori `renewable_energy` memiliki rata-rata `recoveries` tertinggi yang berarti peminjam pada kategori ini cenderung akan gagal bayar (bad debt). Sehingga untuk mengurangi peminjam yang gagal bayar, dapat ditinjau kembali atau ditolak jika user melakukan pinjaman untuk tujuan `renewable_energy` karena risiko kredit akan gagal bayar."""

ma8 = df1.groupby(['target','grade']).agg({'recoveries':'mean'}).reset_index().sort_values(['recoveries'],ascending=False)
ma8.columns = ['target','grade','avg_recoveries']
ma8[ma8['target']==1]

plt.figure(figsize=(10,5))
sns.barplot(x='grade',y='avg_recoveries',data=ma8[ma8['target']==1],color='orange')
plt.text(x=-1,y=1600,s='Berdasarkan peminjam yang gagal bayar\nRata-rata tertinggi dari total rec prncp ada pada grade A',fontsize=14)
plt.show()

ma9 = df1.groupby(['target','grade','purpose']).agg({'recoveries':'mean','annual_inc':'mean','dti':'mean'}).reset_index()
ma9.columns = ['target','grade','purpose','avg_recoveries','avg_annual_inc','avg_dti']
ma9[ma9['target']==1]

plt.figure(figsize=(15,5))
sns.barplot(x='grade',y='avg_recoveries',hue='purpose',data=ma9[ma9['target']==1],palette='coolwarm')
plt.title('Rata-rata recoveries peminjam yang gagal bayar berdasarkan grade dan purpose',fontsize=14)
plt.legend(bbox_to_anchor=(1,1))
plt.show()

"""Jika peminjam yang gagal bayar ditinjau dari feature `grade`, `purpose`, dan rata-rata nilai `recoveries`, maka peminjam yang berada pada kategori `grade` G dengan tujuan untuk energi terbarukan perlu dipertimbangkan untuk ditolak pengajuan kreditnya karena memiliki risiko kredit gagal bayar. begitu pula dengan peminjam yang berada pada kategori `grade` F dengan tujuan peminjaman `wedding` juga perlu dipertimbangkan untuk ditolak pengajuan kreditnya karena mimiliki risiko gagal bayar. Begitu pula pada `grade` B dengan tujuan peminjaman `house` memiliki resiko kredit gagal bayar sehingga dapat dipertimbangkan untuk menolak pengajuan kreditnnya."""

plt.figure(figsize=(15,5))
sns.barplot(x='grade',y='avg_annual_inc',hue='purpose',data=ma9[ma9['target']==1],palette='coolwarm')
plt.title('Rata-rata income tahunan peminjam yang gagal bayar berdasarkan grade dan purpose',fontsize=14)
plt.legend(bbox_to_anchor=(1,1))
plt.show()

z = df1[df1['target']==1][['grade','purpose','dti','recoveries','annual_inc']]
zg = z.groupby(['grade','purpose']).agg({'dti':'mean','recoveries':'mean'}).reset_index().sort_values(['grade','recoveries'],ascending=[True,False])
zg[zg['grade']=='G']

sns.regplot(x='dti',y='recoveries',data=zg)

"""# Cleaning Data"""

df2 = df1.copy()

df2.shape

"""## Check Missing Values"""

#check persentase missing values untuk setiap feature
nv = df2.isnull().sum().sort_values(ascending=False).reset_index()
nv.columns = ['feature','null']
nv['%'] = round(nv['null']*100/(df2.shape[0]),3)
nv = nv[nv['%']>0]
nv

#filter feature yang punya null values > 40%
nv1 = list(nv['feature'][nv['%']>40])
print(nv1)
#filter feature yang null values < 40%
nv2 = list(nv['feature'][nv['%']<40])
print(nv2)

"""## Handling Missing Values

Untuk feature yang memiliki null values > 40% feature tersebut dapat didrop karena tidak representatif lagi. jikalau dilakukan imputation, terlalu banyak mengubah data.<br><br>
Untuk feature yang memiliki null values < 20%, null values akan diisi nilai mendian (untuk data numerik) dan modus (untuk data kategorik) 
"""

# drop feature yang punya null values >40%
df2.drop(columns=nv1,inplace=True)

temp1 = []
temp2 = []
for i in nv2:
  if (df2[i].dtype == 'object') or (df2[i].dtype =='category'):
    temp2.append(i)
  else:
    temp1.append(i)

#check statistical summary from feature on list nv2
df2[temp1].describe().transpose()

"""Karena sebagian besar feature-feature yang memiliki nilai null distribusinya skewed maka nilai untuk mengisi nilai null adalah nilai median dari masing-masing feature."""

#imputation
for i in temp1:
  df2[i].fillna(df2[i].median(),inplace=True)

#check null values
df2[temp1].isnull().sum()

df2[temp2].describe().transpose()

"""Untuk data bertipe kategorik, nilai null pada masing-masing feature diisi dengan nilai modus."""

df2['emp_length'] = df2['emp_length'].fillna('10+ years')
df2['last_pymnt_d'] = df2['last_pymnt_d'].fillna('Jan-16')
df2['last_pymnt_d_month'] = df2['last_pymnt_d_month'].fillna('Jan')
df2['last_pymnt_d_year'] = df2['last_pymnt_d_year'].fillna('2016')
df2['last_credit_pull_d'] = df2['last_credit_pull_d'].fillna('Jan-16')
df2['last_credit_pull_d_month'] = df2['last_credit_pull_d_month'].fillna('Jan')
df2['last_credit_pull_d_year'] = df2['last_credit_pull_d_year'].fillna('2016')
df2['earliest_cr_line'] = df2['earliest_cr_line'].fillna('Oct-00')
df2['earliest_cr_line_month'] = df2['earliest_cr_line_month'].fillna('Oct')
df2['earliest_cr_line_year'] = df2['earliest_cr_line_year'].fillna('2000')

df2[temp2].isnull().sum()

#recheck
df2.isnull().sum().sum()

"""## Check Duplicate Data"""

df2.duplicated().any()

"""Dataset tidak memiliki nilai yang duplikat

# Preprecessing
"""

df2.shape

"""## Feature Engineering : Feature Date

### String to Datetime
"""

df2[['issue_d','last_pymnt_d','last_credit_pull_d','earliest_cr_line']].head()

df2[['issue_d','last_pymnt_d','last_credit_pull_d','earliest_cr_line']].info()

from datetime import datetime as dt
df2['issue_d'] = pd.to_datetime(df2['issue_d'].apply(lambda x: dt.strptime(x, '%b-%y')))
df2['last_pymnt_d'] = pd.to_datetime(df2['last_pymnt_d'].apply(lambda x: dt.strptime(x, '%b-%y')))
df2['last_credit_pull_d'] = pd.to_datetime(df2['last_credit_pull_d'].apply(lambda x: dt.strptime(x, '%b-%y')))
df2['earliest_cr_line'] = pd.to_datetime(df2['earliest_cr_line'].apply(lambda x: dt.strptime(x, '%b-%y')))

df2[['last_pymnt_d','last_credit_pull_d','earliest_cr_line']].info()

df2[['issue_d','last_pymnt_d','last_credit_pull_d','earliest_cr_line']].head()

"""### New Feature

* issue_d : The month which the loan was funded
* last_pymnt_d : Last month payment was received
* last_credit_pull_d : The most recent month LC pulled credit for this loan
* earliest_cr_line : The month the borrower's earliest reported credit line was opened
"""

df2['period'] = (df2['last_credit_pull_d'].dt.year - df2['earliest_cr_line'].dt.year) * 12 + (df2['last_credit_pull_d'].dt.month - df2['earliest_cr_line'].dt.month)
df2['period'].head()

"""## Feature Engineering : Categorical Features"""

nums2 = []
cats2 = []

for i in df2.columns:
  if (df2[i].dtype == 'object') or (df2[i].dtype == 'category'):
    cats2.append(i)
  else:
    nums2.append(i)

print(len(cats2))
print(cats2)

df2.pymnt_plan.value_counts()

"""* Karena feature `grade` dan `sub-grade` merujuk pada hal yang sama maka akan dipilih salah satu untuk proses selanjutnya. Feature yang akan digunakan adalah `grade` karena memiliki label yang lebih sedikit dibandingkan dengan`sub-grade`
* Feature `loan_status` juga akan di drop karena sudah menjadi feature `target`
* Feature `pymnt_plan` di drop karena jumlah label 'n' dan 'y' sangat imbalance. Hal ini akan berpengaruh pada modelling dimana ML akan lebih condong pada label 'n' karena label 'n' mendominasi

"""

temp = ['term', 'grade','emp_length', 'home_ownership','verification_status','purpose','initial_list_status', 
        'issue_d_month', 'issue_d_year', 
        'earliest_cr_line_month', 'earliest_cr_line_year', 
        'last_pymnt_d_month', 'last_pymnt_d_year',
        'last_credit_pull_d_month', 'last_credit_pull_d_year']

len(temp)

plt.figure(figsize=(15,15))
for i in range(0, len(temp)):
    plt.subplot(5, 3, i+1)
    sns.countplot(x=df2[temp[i]].sort_values(), color='royalblue')
    plt.xlabel(temp[i])
    plt.xticks(rotation=90)
    plt.tight_layout()

"""* label 'OWN', 'NONE' dan 'ANY' pada feature `home_ownership` digabungkan dengan label 'OTHER' """

df2.home_ownership.replace({'NONE':'OTHER','ANY':'OTHER','OWN':'OTHER'},inplace=True)
df2.home_ownership.unique()

"""* label pada feature `earliest_cr_line_year` dibuat kelompok baru agar label lebih sedikit dan jumlah masing-masing label tidak terlalu jauh"""

def year(x):
  if int( x['earliest_cr_line_year']) >= 2000:
    result = '00s'
  else:
    if int( x['earliest_cr_line_year']) >= 1990:
      result = '90s'
    elif int( x['earliest_cr_line_year']) >= 1980:
      result = '80s'
    elif int( x['earliest_cr_line_year']) >= 1970:
      result = '70s'
    elif int( x['earliest_cr_line_year']) >= 1960:
      result = '60s'
    else:
      result = '50s'
  return result

df2['earliest_cr_line_year_seg'] = df2.apply(lambda x : year(x),axis=1)
df2[['earliest_cr_line_year','earliest_cr_line_year_seg']].sample(10)

df2.drop(columns='earliest_cr_line_year',inplace=True)

print(temp)

temp1 = ['term', 'grade', 'emp_length', 'home_ownership', 'verification_status', 'purpose', 'initial_list_status','earliest_cr_line_year_seg']

"""## Feature Selection"""

df2[nums2].info()

date = []
temp2 = []
for i in nums2:
  if (df2[i].dtype == 'datetime64[ns]'):
    date.append(i)
  elif (i == 'id') or (i =='member_id'):
    continue
  else:
    temp2.append(i)
print(temp2)

"""## Feature Encoding"""

print(len(temp1))

df_encode = df2.copy()

df_encode[temp1].describe().transpose()

"""### Label Encoding"""

df_encode.term.replace({'36 months':0,'60 months':1},inplace=True)
df_encode.grade.replace({'G':0,'F':1,'E':2,'D':3,'C':4,'B':5,'A':6},inplace=True)
df_encode.emp_length.replace({'< 1 year':0, '1 year':1, '2 years':2,
                                           '3 years':3, '4 years':4, '5 years':5,
                                           '6 years':6, '7 years':7, '8 years':8, 
                                           '9 years':9, '10+ years':10},inplace=True)
df_encode.initial_list_status.replace({'w':0,'f':1},inplace=True)
df_encode.earliest_cr_line_year_seg.replace({'50s':0, '60s':1, '70s':2,
                                           '80s':3, '90s':4, '00s':5},inplace=True)

label_encode = ['term','grade','emp_length','initial_list_status','earliest_cr_line_year_seg']

df_encode[label_encode].head()

"""### One Hot Encoding"""

to_ohe = ['home_ownership','verification_status','purpose']

for cat in to_ohe :
  onehots = pd.get_dummies(df_encode[cat], prefix=cat)
  df_encode = df_encode.join(onehots)

df_encode.columns

df_encode.drop(columns=to_ohe,inplace=True)

result_ohe = ['home_ownership_MORTGAGE', 'home_ownership_OTHER',
       'home_ownership_RENT', 'verification_status_Not Verified',
       'verification_status_Source Verified', 'verification_status_Verified',
       'purpose_car', 'purpose_credit_card', 'purpose_debt_consolidation',
       'purpose_educational', 'purpose_home_improvement', 'purpose_house',
       'purpose_major_purchase', 'purpose_medical', 'purpose_moving',
       'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',
       'purpose_vacation', 'purpose_wedding']

cats_select = label_encode + result_ohe
len(cats_select)

"""## Split Feature and Target"""

df_to_split = df_encode.copy()

select = temp2 + cats_select
len(select)

df_select = df_to_split[select]

df_select.shape

# Split features vs target
X = df_select[[col for col in df_select.columns if (str(df_select[col].dtype) != 'object') and col not in ['target']]]
y = df_select['target'].values 
print(X.shape)
print(y.shape)

"""# Split Train and Test Set"""

X1 = X.copy()
y1 = y.copy()
X2 = X.copy()
y2 = y.copy()

print(X1.shape)
print(y1.shape)

from sklearn.model_selection import train_test_split
# 70 : 30
X1_train1, X1_test1, y1_train1, y1_test1 = train_test_split(X1,y1,test_size=0.3, random_state=1)
# 80 : 20
X1_train2, X1_test2, y1_train2, y1_test2 = train_test_split(X1,y1,test_size=0.2, random_state=1)

print('Split into 70:30')
print('Total rows & column of train set:',X1_train1.shape)
print('Total rows & column of test set:',X1_test1.shape)
print('\nSplit into 80:20')
print('Total rows & column of train set:',X1_train2.shape)
print('Total rows & column of test set:',X1_test2.shape)

"""# Handling Imbalance

## Handling Imbalance Dataset 70:30
"""

a = pd.Series(y1_train1).value_counts().reset_index()
a.columns=['target','total']
a['%'] = round(a['total']*100/sum(a['total']),3)
a

from imblearn import under_sampling, over_sampling
X1_under1, y1_under1 = under_sampling.RandomUnderSampler(0.5).fit_resample(X1_train1, y1_train1)
X1_over1, y1_over1 = over_sampling.RandomOverSampler(0.5).fit_resample(X1_train1, y1_train1)
X1_over_SMOTE1, y1_over_SMOTE1 = over_sampling.SMOTE(0.5).fit_resample(X1_train1, y1_train1)

print('ORIGINAL DATA')
print(pd.Series(y1_train1).value_counts())
print('---'*10, '\n')
print('UNDERSAMPLING DATA')
print(pd.Series(y1_under1).value_counts())
print('---'*10, '\n')
print('OVERSAMPLING DATA')
print(pd.Series(y1_over1).value_counts())
print('---'*10, '\n')
print('SMOTE DATA')
print(pd.Series(y1_over_SMOTE1).value_counts())
print('---'*10, '\n')

X1_train1, y1_train1 = X1_under1, y1_under1

print(X1_train1.shape)
print(y1_train1.shape)

"""## Handling Imbalance Dataset 80:20

"""

b = pd.Series(y1_train2).value_counts().reset_index()
b.columns=['target','total']
b['%'] = round(b['total']*100/sum(b['total']),3)
b

from imblearn import under_sampling, over_sampling
X1_under2, y1_under2 = under_sampling.RandomUnderSampler(0.5).fit_resample(X1_train2, y1_train2)
X1_over2, y1_over2 = over_sampling.RandomOverSampler(0.5).fit_resample(X1_train2, y1_train2)
X1_over_SMOTE2, y1_over_SMOTE2 = over_sampling.SMOTE(0.5).fit_resample(X1_train2, y1_train2)

print('ORIGINAL DATA')
print(pd.Series(y1_train2).value_counts())
print('---'*10, '\n')
print('UNDERSAMPLING DATA')
print(pd.Series(y1_under2).value_counts())
print('---'*10, '\n')
print('OVERSAMPLING DATA')
print(pd.Series(y1_over2).value_counts())
print('---'*10, '\n')
print('SMOTE DATA')
print(pd.Series(y1_over_SMOTE2).value_counts())
print('---'*10, '\n')

X1_train2, y1_train2 = X1_under2, y1_under2

print(X1_train2.shape)
print(y1_train2.shape)

"""## Using Imbalance Dataset"""

print(X2.shape)
print(y2.shape)

from sklearn.model_selection import train_test_split
# 70 : 30
X2_train1, X2_test1, y2_train1, y2_test1 = train_test_split(X2,y2,test_size=0.3, random_state=1)
# 80 : 20
X2_train2, X2_test2, y2_train2, y2_test2 = train_test_split(X2,y2,test_size=0.2, random_state=1)

print('Split into 70:30')
print('Total rows & column of train set:',X2_train1.shape)
print('Total rows & column of test set:',X2_test1.shape)
print('\nSplit into 80:20')
print('Total rows & column of train set:',X2_train2.shape)
print('Total rows & column of test set:',X2_test2.shape)

"""# Modelling"""

#Function Evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, auc

def eval_classification(model,  y_train_pred, y_test_pred, y_train, y_test):
    print("Accuracy (Train Set): %.2f" % accuracy_score(y_train, y_train_pred))
    print("Accuracy (Test Set): %.2f" % accuracy_score(y_test, y_test_pred))
    print("Precision (Train Set): %.2f" % precision_score(y_train, y_train_pred, zero_division=0))
    print("Precision (Test Set): %.2f" % precision_score(y_test, y_test_pred, zero_division=0))
    print("Recall (Train Set): %.2f" % recall_score(y_train, y_train_pred))
    print("Recall (Test Set): %.2f" % recall_score(y_test, y_test_pred))
    print("F1-Score (Train Set): %.2f" % f1_score(y_train, y_train_pred))
    print("F1-Score (Test Set): %.2f" % f1_score(y_test, y_test_pred))
    
    fpr, tpr, thresholds = roc_curve(y_train, y_train_pred, pos_label=1) # pos_label: label yang kita anggap positive
    print("AUC (Train Set): %.2f" % auc(fpr, tpr))
    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1) # pos_label: label yang kita anggap positive
    print("AUC (Test Set): %.2f" % auc(fpr, tpr))

# confusion matrix
from sklearn.metrics import confusion_matrix, plot_confusion_matrix
def cfm(y_test,y_pred):
    cf_matrix = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10,6))
    sns.set(font_scale = 1.5)
    ax = sns.heatmap(cf_matrix, annot=True,fmt = 'd')
    plt.title('Confusion Matrix From Test Set',fontsize=18)
    ax.set_xlabel('Predicted Values')
    ax.set_ylabel('Actual Values')
    plt.show()

"""## Balance Dataset 70:30"""

#check dataset
print(X1_train1.shape)
print(X1_test1.shape)

"""### Default Parameter

#### **Decision Tree**
"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=1)
dt.fit(X1_train1,y1_train1)

y_train_pred = dt.predict(X1_train1)
y_pred = dt.predict(X1_test1)

eval_classification(dt, y_train_pred, y_pred, y1_train1, y1_test1)

#cfm(y1_test1,y_pred)

"""#### **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=1)
rf.fit(X1_train1, y1_train1)

y_train_pred = rf.predict(X1_train1)
y_pred = rf.predict(X1_test1)

eval_classification(rf, y_train_pred, y_pred, y1_train1, y1_test1)

#cfm(y1_test1,y_pred)

"""### Hyperparameter Tuning

#### **Decision Tree**
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
import numpy as np

hyperparameters = dict(max_depth=[5,10,25,50], 
                       min_samples_split=[5,10,25,50,100], 
                       min_samples_leaf=[25,50,100,125],
                       max_features=['auto','sqrt','log2']
                    )
'''max_depth=[5,10,25,50], 
                       min_samples_split=[5,10,25,50,100], 
                       min_samples_leaf=[25,50,100,125],
                       max_features=['auto','sqrt']'''  
dt = DecisionTreeClassifier(random_state=1)
dt_tuned = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=1, scoring='recall')
dt_tuned.fit(X1_train1, y1_train1)

y_train_pred = dt_tuned.predict(X1_train1)
y_pred = dt_tuned.predict(X1_test1)

eval_classification(dt_tuned, y_train_pred, y_pred, y1_train1, y1_test1)

#cfm(y1_test1,y_pred)

"""#### **Random Forest**"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       #n_estimators = [200,300],
                       #bootstrap = [True],
                       #criterion = ['gini','entropy'],
                       max_depth=[5,25,50], 
                       min_samples_split=[25,50,100], 
                       min_samples_leaf=[50,100,125],
                       max_features = ['auto', 'sqrt'])

rf = RandomForestClassifier(random_state=1)
rf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=1, scoring='recall')
rf_tuned.fit(X1_train1,y1_train1)

y_train_pred = rf_tuned.predict(X1_train1)
y_pred = rf_tuned.predict(X1_test1)

eval_classification(rf_tuned, y_train_pred, y_pred, y1_train1, y1_test1)

#cfm(y1_test1,y_pred)

"""## Balance Dataset 80:20"""

#check dataset
print(X1_train2.shape)
print(X1_test2.shape)

"""### Default Parameter

#### **Decision Tree**
"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=1)
dt.fit(X1_train2,y1_train2)

y_train_pred = dt.predict(X1_train2)
y_pred = dt.predict(X1_test2)

eval_classification(dt, y_train_pred, y_pred, y1_train2, y1_test2)

#cfm(y1_test2,y_pred)

"""#### **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=1)
rf.fit(X1_train2, y1_train2)

y_train_pred = rf.predict(X1_train2)
y_pred = rf.predict(X1_test2)

eval_classification(rf, y_train_pred, y_pred, y1_train2, y1_test2)

#cfm(y1_test2,y_pred)

"""### Hyperparameter Tuning

#### **Decision Tree**
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from scipy.stats import uniform
import numpy as np

hyperparameters = dict(max_depth=[5,10,25,50], 
                       min_samples_split=[5,10,25,50,100], 
                       min_samples_leaf=[25,50,100,125],
                       max_features=['auto','sqrt','log2']
                    )

dt = DecisionTreeClassifier(random_state=1)
dt_tuned = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=1, scoring='recall')
dt_tuned.fit(X1_train2, y1_train2)

y_train_pred = dt_tuned.predict(X1_train2)
y_pred = dt_tuned.predict(X1_test2)

eval_classification(dt_tuned, y_train_pred, y_pred, y1_train2, y1_test2)

#cfm(y1_test2,y_pred)

"""#### **Random Forest**"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
hyperparameters = dict(
                       #n_estimators = [200,300],
                       #bootstrap = [True],
                       #criterion = ['gini','entropy'],
                       max_depth=[5,25,50], 
                       min_samples_split=[25,50,100], 
                       min_samples_leaf=[50,100,125],
                       max_features = ['auto', 'sqrt'])

rf = RandomForestClassifier(random_state=1)
rf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=1, scoring='recall')
rf_tuned.fit(X1_train2,y1_train2)

y_train_pred = rf_tuned.predict(X1_train2)
y_pred = rf_tuned.predict(X1_test2)

eval_classification(rf_tuned, y_train_pred, y_pred, y1_train2, y1_test2)

#cfm(y1_test2,y_pred)

"""## Imbalance Dataset 70:30"""

#check dataset
print(X2_train1.shape)
print(X2_test1.shape)

"""### Default Parameter

#### **Decision Tree**
"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=1)
dt.fit(X2_train1,y2_train1)

y_train_pred = dt.predict(X2_train1)
y_pred = dt.predict(X2_test1)

eval_classification(dt, y_train_pred, y_pred, y2_train1, y2_test1)

#cfm(y2_test1,y_pred)

"""#### **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=1)
rf.fit(X2_train1, y2_train1)

y_train_pred = rf.predict(X2_train1)
y_pred = rf.predict(X2_test1)

eval_classification(rf, y_train_pred, y_pred, y2_train1, y2_test1)

#cfm(y2_test1,y_pred)

"""### Hyperparameter Tuning

#### **Decision Tree**
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
import numpy as np

hyperparameters = dict(max_depth=[5,10,25,50], 
                       min_samples_split=[5,10,25,50,100], 
                       min_samples_leaf=[25,50,100,125],
                       max_features=['auto','sqrt','log2']
                    )

dt = DecisionTreeClassifier(random_state=1)
dt_tuned = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=1, scoring='recall')
dt_tuned.fit(X2_train1, y2_train1)

y_train_pred = dt_tuned.predict(X2_train1)
y_pred = dt_tuned.predict(X2_test1)

eval_classification(dt_tuned, y_train_pred, y_pred, y2_train1, y2_test1)

#cfm(y2_test1,y_pred)

"""#### **Random Forest**"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       #n_estimators = [200,300],
                       #bootstrap = [True],
                       #criterion = ['gini','entropy'],
                       max_depth=[5,25,50], 
                       min_samples_split=[25,50,100], 
                       min_samples_leaf=[50,100,125],
                       max_features = ['auto', 'sqrt'])

rf = RandomForestClassifier(random_state=1)
rf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=1, scoring='recall')
rf_tuned.fit(X2_train1,y2_train1)

y_train_pred = rf_tuned.predict(X2_train1)
y_pred = rf_tuned.predict(X2_test1)

eval_classification(rf_tuned, y_train_pred, y_pred, y2_train1, y2_test1)

def show_feature_importance(model):
    feat_importances = pd.Series(model.best_estimator_.feature_importances_, index=X.columns)
    ax = feat_importances.nlargest(10).plot(kind='barh', figsize=(10, 8))
    ax.invert_yaxis()
    plt.xlabel('score')
    plt.ylabel('feature')
    plt.title('feature importance score')

def show_best_hyperparameter(model, hyperparameters):
    for key, value in hyperparameters.items() :
        print('Best '+key+':', model.get_params()[key])

show_best_hyperparameter(rf_tuned.best_estimator_, hyperparameters)

show_feature_importance(rf_tuned)



#cfm(y2_test1,y_pred)

"""## Imbalance Dataset 80:20"""

#check dataset
print(X2_train2.shape)
print(X2_test2.shape)

"""### Default Parameter

#### **Decision Tree**
"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=1)
dt.fit(X2_train2,y2_train2)

y_train_pred = dt.predict(X2_train2)
y_pred = dt.predict(X2_test2)

eval_classification(dt, y_train_pred, y_pred, y2_train2, y2_test2)

#cfm(y2_test2,y_pred)

"""#### **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=1)
rf.fit(X2_train2, y2_train2)

y_train_pred = rf.predict(X2_train2)
y_pred = rf.predict(X2_test2)

eval_classification(rf, y_train_pred, y_pred, y2_train2, y2_test2)

#cfm(y2_test2,y_pred)

"""### Hyperparameter Tuning

#### **Decision Tree**
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
import numpy as np

hyperparameters = dict(max_depth=[5,10,25,50], 
                       min_samples_split=[5,10,25,50,100], 
                       min_samples_leaf=[25,50,100,125],
                       max_features=['auto','sqrt','log2']
                    )

dt = DecisionTreeClassifier(random_state=1)
dt_tuned = RandomizedSearchCV(dt, hyperparameters, cv=5, random_state=1, scoring='recall')
dt_tuned.fit(X2_train2, y2_train2)

y_train_pred = dt_tuned.predict(X2_train2)
y_pred = dt_tuned.predict(X2_test2)

eval_classification(dt_tuned, y_train_pred, y_pred, y2_train2, y2_test2)

"""#### **Random Forest**"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

#List Hyperparameters yang akan diuji
hyperparameters = dict(
                       #n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 20)],
                       #bootstrap = [True],
                       #criterion = ['gini','entropy'],
                       max_depth=[5,10,25,50], 
                       min_samples_split=[5,10,25,50,100], 
                       min_samples_leaf=[25,50,100,125],
                       max_features = ['auto', 'sqrt'])

rf = RandomForestClassifier(random_state=1)
rf_tuned = RandomizedSearchCV(rf, hyperparameters, cv=5, random_state=1, scoring='recall')
rf_tuned.fit(X2_train2,y2_train2)

y_train_pred = rf_tuned.predict(X2_train2)
y_pred = rf_tuned.predict(X2_test2)

eval_classification(rf_tuned, y_train_pred, y_pred, y2_train2, y2_test2)